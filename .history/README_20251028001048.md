# ResuAI - University Personnel Assessment System

ResuAI is a comprehensive university personnel assessment and recruitment management system designed specifically for academic institutions. It combines advanced AI-powered document processing with sophisticated assessment workflows to streamline faculty and staff hiring processes.

## ğŸ“ System Overview

ResuAI is designed for **university HR departments** and **academic recruitment teams** to:
- Process Personal Data Sheets (PDS) and resumes automatically
- Assess candidates using university-specific scoring criteria
- Manage job postings and position requirements
- Track candidates through the hiring pipeline
- Generate detailed assessment reports and rankings

## ğŸš€ Core Features

### ğŸ“„ Document Processing Engine
- **PDS Extraction**: Specialized processing for Philippine Civil Service Form 212 (Personal Data Sheet)
- **Resume Processing**: Support for PDF, XLSX, DOCX, and TXT files
- **Intelligent Detection**: Automatically identifies document types and extracts relevant data
- **Batch Processing**: Handle multiple documents simultaneously with progress tracking
- **Error Recovery**: Fallback mechanisms for problematic documents

### ğŸ¯ University Assessment Engine
- **Multi-Category Scoring**: Education (40%), Experience (20%), Training (10%), Eligibility (10%), Accomplishments (5%), Potential (15%)
- **Position-Specific Criteria**: Customizable assessment templates for different university positions
- **Automated Calculations**: Real-time scoring with detailed breakdowns
- **Manual Override**: Assessment officers can adjust scores and add notes
- **Ranking System**: Automatic candidate ranking with recommendation levels

### ğŸ‘¥ Candidate Management
- **Comprehensive Profiles**: Complete candidate information with extracted PDS data
- **Assessment History**: Track all assessments and score changes over time
- **Status Tracking**: Monitor candidates through hiring stages
- **Document Storage**: Organized file management with original documents
- **Batch Operations**: Process multiple candidates efficiently

### ğŸ’¼ Job & Position Management
- **Job Posting System**: Create and manage university position announcements
- **Position Requirements**: Define education, experience, and certification requirements
- **Category Management**: Organize positions by department, level, and type
- **Assessment Templates**: Link specific scoring criteria to position types
- **Requirements Matching**: Automatic candidate-to-position compatibility scoring

### ğŸ” User Authentication & Access Control
- **Role-Based System**: Admin, Assessment Officer, and Viewer roles
- **Secure Authentication**: Session-based login with password hashing
- **User Management**: Admin can create, edit, and manage user accounts
- **Activity Logging**: Track user actions and system usage
- **Department-Level Access**: Restrict access based on organizational units

### ğŸ“Š Analytics & Reporting
- **Assessment Analytics**: Detailed scoring statistics and trends
- **Candidate Reports**: Comprehensive assessment summaries
- **Comparison Tools**: Side-by-side candidate evaluations
- **Export Functionality**: Generate reports in multiple formats
- **Dashboard Insights**: Real-time system status and metrics

## ï¿½ï¸ Technical Architecture

### System Components

**Backend Framework:**

- **Flask Application**: Core web framework with class-based architecture
- **Database Layer**: Hybrid SQLite/PostgreSQL support with database manager
- **Assessment Engine**: University-specific scoring algorithms
- **Document Processor**: Multi-format file parsing and data extraction
- **Clean Upload Handler**: Batch file processing with session management

**AI & NLP Components:**

- **spaCy**: Natural language processing and entity recognition
- **BERT Models**: Semantic understanding with DistilBERT
- **Sentence Transformers**: Document similarity and matching
- **NLTK**: Text preprocessing and analysis
- **FAISS**: Vector similarity search and indexing
- **scikit-learn**: Classification and scoring algorithms

**Database Schema:**

- **candidates**: Core candidate information and assessment data
- **candidate_assessments**: Detailed assessment records and scoring
- **lspu_job_postings**: University position announcements
- **users**: Authentication and user management
- **position_types**: Position categories and requirements
- **assessment_templates**: Scoring criteria definitions
- **upload_sessions**: File processing tracking

### File Processing Capabilities

**Supported Formats:**

- **PDF**: Personal Data Sheets, resumes, certificates
- **XLSX**: Structured data extraction from Excel files
- **DOCX**: Microsoft Word documents
- **TXT**: Plain text files

**Processing Features:**

- **Smart Detection**: Automatically identifies PDS vs. resume format
- **Data Extraction**: Parses education, experience, training, eligibility
- **Error Handling**: Robust fallback mechanisms for corrupted files
- **Batch Processing**: Multiple file uploads with progress tracking
- **Preview Generation**: File content preview before processing

## ğŸ“‹ System Requirements

### Server Requirements

**Minimum Specifications:**

- **CPU**: 2+ cores, 2.5GHz
- **RAM**: 4GB (8GB recommended for better performance)
- **Storage**: 10GB free space (database + uploaded files)
- **OS**: Windows 10+, macOS 10.15+, Ubuntu 18.04+

**Recommended Specifications:**

- **CPU**: 4+ cores, 3.0GHz
- **RAM**: 8GB+ (for multiple concurrent users)
- **Storage**: 50GB+ SSD storage
- **Network**: Stable internet connection for AI model downloads

### Software Dependencies

**Core Requirements:**

- **Python 3.8+** (3.9 or 3.10 recommended)
- **SQLite** (included with Python) OR **PostgreSQL 12+**
- **pip** package manager

**AI Model Storage:**

- **BERT Models**: ~500MB download
- **spaCy Models**: ~50MB download
- **NLTK Data**: ~100MB download

## ğŸ”§ Installation & Setup

### Quick Start (Development)

#### Step 1: Clone Repository

```bash
git clone https://github.com/Rainieeer/ResuAI.git
cd ResuAI
```

#### Step 2: Setup Python Environment

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate
```

#### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

#### Step 4: Initialize Database

```bash
# Run the application (will auto-create SQLite database)
python app.py
```

#### Step 5: Access System

Open browser to: `http://localhost:5000`

**Default Admin Account:**

- **Email**: `admin@example.com`
- **Password**: `admin123`

### Production Setup

#### Database Configuration

**For SQLite (Development):**

- Automatic setup - no configuration needed
- Database file: `resume_screening.db`
- Suitable for single-user or testing

**For PostgreSQL (Production):**

1. **Install PostgreSQL server**
2. **Create database:**

```sql
CREATE DATABASE resumai_university;
CREATE USER resumai_user WITH PASSWORD 'secure_password';
GRANT ALL PRIVILEGES ON DATABASE resumai_university TO resumai_user;
```

3. **Configure environment variables:**

```env
DATABASE_URL=postgresql://resumai_user:secure_password@localhost:5432/resumai_university
FLASK_ENV=production
SECRET_KEY=your-production-secret-key
```

#### Web Server Deployment

**Using Gunicorn (Recommended):**

```bash
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 --timeout 300 app:app
```

**Using Docker:**

```dockerfile
FROM python:3.9-slim
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
EXPOSE 5000
CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
```

## ğŸ“ Project Structure

```
ResuAI/
â”œâ”€â”€ app.py                           # Main Flask application (5920 lines)
â”œâ”€â”€ database.py                      # Database management layer
â”œâ”€â”€ assessment_engine.py             # University assessment algorithms
â”œâ”€â”€ utils.py                         # Document processing utilities (2752 lines)
â”œâ”€â”€ clean_upload_handler.py          # File upload management
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”‚
â”œâ”€â”€ Database Schema Files/
â”‚   â”œâ”€â”€ schema.sql                   # Main database schema
â”‚   â”œâ”€â”€ assessment_schema.sql        # Assessment tables
â”‚   â”œâ”€â”€ pds_schema.sql              # PDS-specific tables
â”‚   â”œâ”€â”€ enhanced_candidates_schema.sql # Extended candidate data
â”‚   â””â”€â”€ lspu_job_posting_schema.sql # Job posting structure
â”‚
â”œâ”€â”€ static/                          # Frontend assets
â”‚   â”œâ”€â”€ css/                         # Stylesheets
â”‚   â”‚   â”œâ”€â”€ base.css                 # Core styling
â”‚   â”‚   â”œâ”€â”€ components/              # UI component styles
â”‚   â”‚   â””â”€â”€ pages/                   # Page-specific styles
â”‚   â”œâ”€â”€ js/                          # JavaScript modules
â”‚   â”‚   â”œâ”€â”€ modules/                 # Feature modules
â”‚   â”‚   â”œâ”€â”€ services/                # API services
â”‚   â”‚   â””â”€â”€ utils/                   # Utility functions
â”‚   â””â”€â”€ data/                        # Static data files
â”‚
â”œâ”€â”€ templates/                       # HTML templates
â”‚   â”œâ”€â”€ dashboard.html               # Main application interface
â”‚   â”œâ”€â”€ index.html                   # Landing page
â”‚   â”œâ”€â”€ login.html                   # Authentication
â”‚   â”œâ”€â”€ user_management.html         # Admin user management
â”‚   â””â”€â”€ job_posting_management.html  # Job management interface
â”‚
â”œâ”€â”€ uploads/                         # Uploaded document storage
â”œâ”€â”€ temp_uploads/                    # Temporary file processing
â”œâ”€â”€ instance/                        # Instance-specific configurations
â”‚
â””â”€â”€ SampleTestFiles/                 # Test documents and examples
    â”œâ”€â”€ PDS samples/
    â”œâ”€â”€ Resume samples/
    â””â”€â”€ Test data/
```

## ğŸ”’ Security & Best Practices

### Environment Security
- **Never commit `.env` files** - they're in `.gitignore` for a reason
- **Use strong passwords** for database connections
- **Change the SECRET_KEY** before deploying to production
- **Regularly update dependencies** to patch security vulnerabilities

### Database Security
```sql
-- Create a dedicated user for the application
CREATE USER resumai_app WITH PASSWORD 'strong_random_password';
GRANT CONNECT ON DATABASE resumai_db TO resumai_app;
GRANT USAGE ON SCHEMA public TO resumai_app;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO resumai_app;
```

## ğŸ¤ Team Collaboration

### Recommended Setup for Teams

#### Option 1: Local Development (Recommended)
Each developer has their own local setup:
- Individual PostgreSQL databases
- Separate `.env` files
- Independent development environments

#### Option 2: Shared Development Database
Use a cloud database service:
- **Railway**: Easy PostgreSQL hosting
- **Supabase**: Free PostgreSQL with web interface
- **AWS RDS**: Production-grade database hosting
- **Google Cloud SQL**: Scalable database solution

#### Team Workflow
1. **Clone the repository**
2. **Create your own `.env` file** (never commit it)
3. **Set up local database** or use shared development database
4. **Run `semantic_setup.py`** to download AI models
5. **Start development** with `python app.py`

## ğŸš€ Deployment

### Production Deployment Checklist

1. **Environment Configuration**
   ```env
   FLASK_ENV=production
   FLASK_DEBUG=False
   SECRET_KEY=your-production-secret-key
   DATABASE_URL=postgresql://user:password@your-production-db:5432/resumai_db
   ```

2. **Database Setup**
   - Use a production PostgreSQL instance
   - Enable SSL connections
   - Set up regular backups
   - Configure connection pooling

3. **Web Server**
   - Use Gunicorn or uWSGI instead of Flask's built-in server
   - Set up reverse proxy with Nginx
   - Configure SSL certificates

4. **Monitoring & Logging**
   - Set up application monitoring
   - Configure log aggregation
   - Set up error tracking

### Quick Deploy with Gunicorn
```bash
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

## ğŸ“Š Usage Guide

### First Steps After Setup

1. **Access the Dashboard**: Navigate to `http://localhost:5000`
2. **Create Job Categories**: Set up job categories and requirements
3. **Upload Resumes**: Use the upload interface to add candidate resumes
4. **Review Results**: Check AI-generated match scores and recommendations
5. **Manage Candidates**: Organize and track candidates through the hiring process

### API Endpoints

The application provides REST API endpoints:
- `POST /upload` - Upload resume files
- `GET /candidates` - Retrieve candidate data
- `GET /jobs` - Get job listings
- `GET /analytics` - Analytics data
- `GET /api/candidates/<id>` - Individual candidate details

## ğŸ¤– AI Model Information

### Models Used
- **Sentence Transformers**: `all-MiniLM-L6-v2` for semantic similarity
- **DistilBERT**: `distilbert-base-uncased` for text understanding
- **spaCy**: `en_core_web_sm` for NLP processing
- **Custom TF-IDF**: Trained on resume and job description data
- **Random Forest**: For classification and recommendation

### Model Performance
- **Semantic Similarity**: Threshold of 0.7 for matching
- **Processing Speed**: ~2-3 seconds per resume
- **Memory Usage**: ~1GB for full model loading
- **Accuracy**: 85%+ for job-candidate matching

## ğŸ“ˆ Performance Optimization

### For Better Performance
- **Increase BERT_MAX_LENGTH** for longer documents (up to 512)
- **Adjust cache sizes** based on available memory
- **Use GPU acceleration** for faster model inference
- **Implement caching** for frequently accessed data

### Memory Management
```env
# Adjust these based on your system capabilities
MAX_EMBEDDING_CACHE=1000      # Increase for more caching
MAX_SIMILARITY_CACHE=2000     # Increase for better performance
BERT_MAX_LENGTH=256           # Increase for longer text processing
```

## ğŸ†˜ Support

### Getting Help
1. **Check the troubleshooting section** above
2. **Review the error logs** in the terminal
3. **Test individual components** using the provided test scripts
4. **Check GitHub issues** for similar problems

### Contributing
Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

MIT License - See LICENSE file for details.

---

**Note**: This is an AI-powered application that requires significant computational resources. For production use, ensure adequate server specifications and consider implementing rate limiting and resource monitoring.
