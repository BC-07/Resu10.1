# üß† Semantic Scoring System - Complete Technical Documentation

## Table of Contents
1. [System Overview](#system-overview)
2. [Core Components](#core-components)
3. [Database Schema](#database-schema)
4. [Scoring Algorithm](#scoring-algorithm)
5. [Configuration & Customization](#configuration--customization)
6. [Integration Points](#integration-points)
7. [Performance & Optimization](#performance--optimization)
8. [Troubleshooting](#troubleshooting)
9. [Extension & Modification Guide](#extension--modification-guide)

---

## System Overview

The semantic scoring system is an AI-powered candidate assessment engine that uses natural language processing to evaluate how well a candidate matches a job posting. Unlike traditional keyword-based matching, it understands the meaning and context of text to provide more accurate assessments.

### Key Features:
- **Semantic Understanding**: Uses sentence-transformers to understand meaning, not just keywords
- **Dual Scoring**: Provides both semantic and traditional scores for comparison
- **Real-time Processing**: Fast assessment with sub-second response times
- **Scalable**: Handles batch processing for multiple candidates
- **Configurable**: Adjustable weights and parameters for different job types

---

## Core Components

### 1. Semantic Engine (`semantic_engine.py`)

**Purpose**: Core AI engine that handles text encoding and similarity calculations.

**Key Class**: `UniversitySemanticEngine`

```python
# Initialize the engine
engine = UniversitySemanticEngine()

# Encode text to vector embeddings
job_embedding = engine.encode_job_requirements(job_data)
candidate_embedding = engine.encode_candidate_profile(candidate_data)

# Calculate similarity
similarity = engine.calculate_similarity(job_embedding, candidate_embedding)
```

**Key Methods**:
- `encode_job_requirements()`: Converts job posting to 384-dimensional vector
- `encode_candidate_profile()`: Converts resume/profile to 384-dimensional vector
- `calculate_similarity()`: Computes cosine similarity between vectors
- `get_detailed_semantic_score()`: Provides comprehensive scoring with breakdown

**Technology Stack**:
- **Model**: `all-MiniLM-L6-v2` (384-dimensional embeddings)
- **Framework**: sentence-transformers
- **Search**: FAISS CPU indexing for efficient similarity search
- **Performance**: Embedding caching to avoid recomputation

### 2. Enhanced Assessment Engine (`enhanced_assessment_engine.py`)

**Purpose**: Integration layer that combines semantic and traditional assessment methods.

**Key Class**: `EnhancedUniversityAssessmentEngine`

```python
# Initialize enhanced engine
enhanced_engine = EnhancedUniversityAssessmentEngine(db_manager=db_manager)

# Assess a single candidate
result = enhanced_engine.assess_candidate_enhanced(
    candidate_data=candidate,
    job_data=job_posting
)

# Batch assess multiple candidates
batch_results = enhanced_engine.batch_assess_candidates(
    candidates_data=candidates_list,
    job_data=job_posting
)
```

**Assessment Modes**:
- `'semantic_only'`: Uses only semantic scoring
- `'traditional_only'`: Uses only traditional scoring  
- `'hybrid'`: Combines both methods (default)

---

## Database Schema

### Primary Table: `candidates`

The semantic scoring system adds three new fields to the existing candidates table:

```sql
-- New semantic scoring fields
ALTER TABLE candidates ADD COLUMN semantic_score FLOAT DEFAULT 0.0;
ALTER TABLE candidates ADD COLUMN semantic_breakdown JSONB DEFAULT '{}'::jsonb;
ALTER TABLE candidates ADD COLUMN semantic_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP;

-- Performance indexes
CREATE INDEX idx_candidates_semantic_score ON candidates(semantic_score);
CREATE INDEX idx_candidates_semantic_breakdown ON candidates USING GIN(semantic_breakdown);
```

### Field Descriptions:

#### `semantic_score` (FLOAT)
- **Range**: 0.0 to 100.0
- **Purpose**: Final semantic assessment score
- **Usage**: Primary score for ranking and filtering candidates
- **Example**: `75.5` means 75.5% semantic match

#### `semantic_breakdown` (JSONB)
- **Purpose**: Detailed scoring components and metadata
- **Structure**: JSON object with scoring details
- **Example**:
```json
{
  "education_relevance": 0.852,
  "experience_relevance": 0.743,
  "skills_relevance": 0.691,
  "overall_similarity": 0.758,
  "weighted_components": {
    "education_weighted": 0.298,
    "experience_weighted": 0.334,
    "skills_weighted": 0.104,
    "quality_bonus": 0.038
  },
  "final_calculation": {
    "base_weighted_score": 0.736,
    "quality_bonus": 0.038,
    "final_score_0_1": 0.774,
    "final_score_0_100": 77.4
  },
  "weights_used": {
    "education_relevance": 0.35,
    "experience_relevance": 0.45,
    "skills_relevance": 0.15,
    "overall_quality_bonus": 0.05
  },
  "model_info": {
    "model_name": "all-MiniLM-L6-v2",
    "similarity_threshold": 0.3
  }
}
```

#### `semantic_updated` (TIMESTAMP)
- **Purpose**: Track when semantic scoring was last performed
- **Usage**: Cache invalidation, audit trails, data freshness
- **Auto-updated**: Yes, whenever semantic score is recalculated

### Database Queries Examples:

```sql
-- Get top 10 candidates by semantic score
SELECT name, semantic_score, semantic_breakdown->'final_calculation'->>'final_score_0_100' as score
FROM candidates 
WHERE semantic_score IS NOT NULL 
ORDER BY semantic_score DESC 
LIMIT 10;

-- Find candidates with high education relevance
SELECT name, semantic_score
FROM candidates 
WHERE semantic_breakdown->'education_relevance' > '0.8'::numeric;

-- Get candidates assessed in last 24 hours
SELECT name, semantic_score, semantic_updated
FROM candidates 
WHERE semantic_updated > NOW() - INTERVAL '24 hours';

-- Filter by specific scoring components
SELECT name, semantic_breakdown->'skills_relevance' as skills_match
FROM candidates 
WHERE semantic_breakdown->'skills_relevance' > '0.7'::numeric;
```

---

## Scoring Algorithm

### Scoring Components & Weights

The semantic scoring algorithm evaluates four main components:

1. **Education Relevance** (35% weight)
   - Compares candidate's education background with job requirements
   - Considers degree level, field of study, institution relevance
   
2. **Experience Relevance** (45% weight)
   - Analyzes work history against job requirements
   - Considers role similarities, industry match, responsibility level
   
3. **Skills Relevance** (15% weight)
   - Matches technical and soft skills
   - Includes programming languages, tools, methodologies
   
4. **Overall Quality Bonus** (5% weight)
   - Rewards high-quality, well-structured content
   - Considers completeness, clarity, professionalism

### Calculation Process:

```python
# Step 1: Calculate component similarities (0-1 scale)
education_sim = cosine_similarity(job_education_embedding, candidate_education_embedding)
experience_sim = cosine_similarity(job_experience_embedding, candidate_experience_embedding)
skills_sim = cosine_similarity(job_skills_embedding, candidate_skills_embedding)
overall_sim = cosine_similarity(job_full_embedding, candidate_full_embedding)

# Step 2: Apply weights
weighted_score = (
    education_sim * 0.35 +
    experience_sim * 0.45 + 
    skills_sim * 0.15
)

# Step 3: Add quality bonus
quality_bonus = overall_sim * 0.05
final_score_0_1 = weighted_score + quality_bonus

# Step 4: Convert to 0-100 scale
final_score = final_score_0_1 * 100
```

### Similarity Threshold:
- **Minimum**: 0.3 (30% similarity required for relevance)
- **Below threshold**: Component scores are set to 0
- **Purpose**: Filters out completely irrelevant content

---

## Configuration & Customization

### 1. Model Configuration

**Location**: `semantic_engine.py` - `UniversitySemanticEngine.__init__()`

```python
class UniversitySemanticEngine:
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        # Change model for different performance/accuracy trade-offs
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        
        # FAISS configuration
        self.dimension = 384  # Model-specific
        self.faiss_index = None
        
        # Caching configuration
        self.cache_embeddings = True
        self.embedding_cache = {}
```

**Alternative Models**:
- `all-MiniLM-L6-v2` (384 dim): Current default - fast, lightweight
- `all-mpnet-base-v2` (768 dim): Higher accuracy, slower
- `distilbert-base-nli-mean-tokens` (768 dim): Good balance
- `paraphrase-multilingual-MiniLM-L12-v2` (384 dim): Multilingual support

### 2. Scoring Weights Configuration

**Location**: `semantic_engine.py` - `get_detailed_semantic_score()`

```python
# Default weights - modify these for different job types
weights = {
    'education_relevance': 0.35,    # 35% - academic background importance
    'experience_relevance': 0.45,   # 45% - work experience importance  
    'skills_relevance': 0.15,       # 15% - technical skills importance
    'overall_quality_bonus': 0.05   # 5% - content quality bonus
}

# Example: Technical role weights
tech_weights = {
    'education_relevance': 0.20,    # Less emphasis on formal education
    'experience_relevance': 0.50,   # High emphasis on experience
    'skills_relevance': 0.25,       # Higher emphasis on technical skills
    'overall_quality_bonus': 0.05
}

# Example: Academic role weights
academic_weights = {
    'education_relevance': 0.50,    # High emphasis on education
    'experience_relevance': 0.30,   # Moderate experience emphasis
    'skills_relevance': 0.15,       # Standard skills emphasis
    'overall_quality_bonus': 0.05
}
```

### 3. Assessment Mode Configuration

**Location**: `enhanced_assessment_engine.py` - `assess_candidate_enhanced()`

```python
# Available assessment modes
assessment_modes = {
    'semantic_only': 'Use only semantic scoring',
    'traditional_only': 'Use only traditional scoring',
    'hybrid': 'Combine both methods (default)'
}

# Configure default mode
result = enhanced_engine.assess_candidate_enhanced(
    candidate_data=candidate,
    job_data=job_posting,
    assessment_mode='hybrid'  # Change this
)
```

### 4. Performance Configuration

**FAISS Index Settings**:
```python
# In semantic_engine.py
class UniversitySemanticEngine:
    def _initialize_faiss_index(self):
        # For small datasets (< 10k candidates)
        index = faiss.IndexFlatIP(self.dimension)  # Exact search
        
        # For large datasets (> 10k candidates)
        # index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
        # index.train(training_embeddings)
        
        return index
```

**Caching Settings**:
```python
# Enable/disable embedding caching
self.cache_embeddings = True  # Set to False to disable caching
self.max_cache_size = 1000    # Maximum cached embeddings
```

---

## Integration Points

### 1. API Integration

**Main Entry Point**: Enhanced Assessment Engine

```python
from enhanced_assessment_engine import EnhancedUniversityAssessmentEngine

# Initialize
engine = EnhancedUniversityAssessmentEngine()

# Single candidate assessment
result = engine.assess_candidate_enhanced(
    candidate_data={
        'id': 123,
        'name': 'John Doe',
        'extracted_text': 'Resume text here...',
        'education': 'BS Computer Science',
        'experience': '5 years software development',
        'skills': 'Python, React, SQL'
    },
    job_data={
        'title': 'Senior Software Engineer',
        'description': 'Job description here...',
        'requirements': ['BS degree', 'Python experience', 'Team leadership']
    }
)

# Result structure
{
    'total_score': 75.5,           # Final combined score
    'semantic_score': 77.4,        # Semantic component score
    'traditional_score': 73.6,     # Traditional component score (if available)
    'semantic_breakdown': {...},   # Detailed semantic analysis
    'assessment_mode': 'hybrid',   # Mode used for assessment
    'timestamp': '2025-10-28T15:30:00'
}
```

### 2. Database Integration

**Automatic Updates**: Scores are automatically saved to database during assessment

```python
# The enhanced engine automatically updates the database
# No manual database calls needed

# But you can also manually update:
conn = db_manager.get_connection()
cursor = conn.cursor()

cursor.execute("""
    UPDATE candidates 
    SET semantic_score = %s, 
        semantic_breakdown = %s,
        semantic_updated = %s
    WHERE id = %s
""", (
    result['semantic_score'],
    json.dumps(result['semantic_breakdown']),
    datetime.now(),
    candidate_id
))

conn.commit()
cursor.close()
conn.close()
```

### 3. Web Application Integration

**Flask Route Example**:
```python
from flask import request, jsonify
from enhanced_assessment_engine import get_enhanced_assessment_engine

@app.route('/api/assess_candidate', methods=['POST'])
def assess_candidate():
    data = request.get_json()
    
    engine = get_enhanced_assessment_engine()
    
    result = engine.assess_candidate_enhanced(
        candidate_data=data['candidate'],
        job_data=data['job_posting']
    )
    
    return jsonify(result)
```

---

## Performance & Optimization

### Current Performance Metrics:
- **Single Assessment**: ~0.035 seconds
- **Batch Processing**: ~0.001 seconds per candidate (amortized)
- **Embedding Generation**: ~0.015 seconds per text block
- **Similarity Calculation**: ~0.022 seconds
- **Memory Usage**: ~200MB for model + embeddings cache

### Optimization Strategies:

#### 1. Embedding Caching
```python
# Embeddings are cached to avoid recomputation
# Cache hit = instant retrieval
# Cache miss = compute once, store for future use

# View cache status
engine = UniversitySemanticEngine()
print(f"Cache size: {len(engine.embedding_cache)}")
print(f"Cache enabled: {engine.cache_embeddings}")
```

#### 2. Batch Processing
```python
# Process multiple candidates efficiently
results = engine.batch_assess_candidates(
    candidates_data=candidates_list,  # List of candidate dicts
    job_data=job_posting
)

# Vectorized operations reduce per-candidate overhead
# FAISS indexing enables fast similarity search
```

#### 3. Database Optimization
```sql
-- Use indexes for fast querying
EXPLAIN ANALYZE SELECT * FROM candidates WHERE semantic_score > 75;

-- Use JSONB operators for semantic_breakdown queries
SELECT * FROM candidates 
WHERE semantic_breakdown @> '{"education_relevance": 0.8}';

-- Limit result sets for large tables
SELECT * FROM candidates 
ORDER BY semantic_score DESC 
LIMIT 50 OFFSET 0;
```

### Scaling Considerations:

**For 1K-10K candidates**: Current setup is optimal
**For 10K-100K candidates**: Consider FAISS IVF indexing
**For 100K+ candidates**: Consider distributed vector database (Pinecone, Weaviate)

```python
# Large-scale FAISS configuration
def setup_large_scale_index(dimension, training_data):
    quantizer = faiss.IndexFlatIP(dimension)
    index = faiss.IndexIVFFlat(quantizer, dimension, nlist=100)
    index.train(training_data)
    return index
```

---

## Troubleshooting

### Common Issues & Solutions:

#### 1. Import Errors
```bash
# Error: No module named 'sentence_transformers'
pip install sentence-transformers

# Error: No module named 'faiss'
pip install faiss-cpu

# Error: CUDA issues
pip uninstall faiss-gpu
pip install faiss-cpu
```

#### 2. Model Loading Issues
```python
# Error: Model download fails
# Solution: Manual download and specify local path
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2', cache_folder='./models/')
```

#### 3. Database Connection Issues
```python
# Error: Database connection fails
# Check database configuration in DatabaseManager
db_manager = DatabaseManager()
print(f"Using SQLite: {db_manager.use_sqlite}")

# Test connection
try:
    conn = db_manager.get_connection()
    print("‚úÖ Database connection successful")
    conn.close()
except Exception as e:
    print(f"‚ùå Database connection failed: {e}")
```

#### 4. Performance Issues
```python
# Issue: Slow embedding generation
# Solution: Enable caching
engine.cache_embeddings = True

# Issue: High memory usage
# Solution: Limit cache size
engine.max_cache_size = 500

# Issue: Slow similarity search
# Solution: Use FAISS indexing for large datasets
```

#### 5. Scoring Issues
```python
# Issue: All scores are 0
# Check similarity threshold
if similarity < engine.similarity_threshold:
    print("Similarity below threshold, check content relevance")

# Issue: Unexpected scores
# Check semantic breakdown for component analysis
print(json.dumps(result['semantic_breakdown'], indent=2))
```

### Debug Mode:
```python
# Enable verbose logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Test individual components
engine = UniversitySemanticEngine()

# Test encoding
job_emb = engine.encode_job_requirements(job_data)
cand_emb = engine.encode_candidate_profile(candidate_data)
print(f"Job embedding shape: {job_emb.shape}")
print(f"Candidate embedding shape: {cand_emb.shape}")

# Test similarity
similarity = engine.calculate_similarity(job_emb, cand_emb)
print(f"Similarity: {similarity}")
```

---

## Extension & Modification Guide

### 1. Adding New Scoring Components

**Step 1**: Modify `get_detailed_semantic_score()` in `semantic_engine.py`

```python
def get_detailed_semantic_score(self, job_data: Dict, candidate_data: Dict) -> Dict:
    # Existing components...
    
    # ADD NEW COMPONENT
    certifications_relevance = self._calculate_certifications_relevance(
        job_data, candidate_data
    )
    
    # Update weights (ensure they sum to 1.0)
    weights = {
        'education_relevance': 0.30,      # Reduced from 0.35
        'experience_relevance': 0.40,     # Reduced from 0.45
        'skills_relevance': 0.15,         # Same
        'certifications_relevance': 0.10, # NEW
        'overall_quality_bonus': 0.05     # Same
    }
    
    # Update calculation
    weighted_score = (
        education_relevance * weights['education_relevance'] +
        experience_relevance * weights['experience_relevance'] +
        skills_relevance * weights['skills_relevance'] +
        certifications_relevance * weights['certifications_relevance']  # NEW
    )
```

**Step 2**: Implement the new component calculation

```python
def _calculate_certifications_relevance(self, job_data: Dict, candidate_data: Dict) -> float:
    """Calculate relevance of candidate certifications to job requirements"""
    
    # Extract certification requirements from job
    job_cert_text = self._extract_certification_requirements(job_data)
    
    # Extract candidate certifications
    candidate_cert_text = self._extract_candidate_certifications(candidate_data)
    
    if not job_cert_text or not candidate_cert_text:
        return 0.0
    
    # Encode and calculate similarity
    job_cert_emb = self.model.encode(job_cert_text)
    cand_cert_emb = self.model.encode(candidate_cert_text)
    
    similarity = self.calculate_similarity(job_cert_emb, cand_cert_emb)
    
    return max(0.0, similarity) if similarity >= self.similarity_threshold else 0.0
```

### 2. Adding New Assessment Modes

**Modify**: `enhanced_assessment_engine.py`

```python
def assess_candidate_enhanced(self, candidate_data: Dict, job_data: Dict, 
                            assessment_mode: str = 'hybrid') -> Dict:
    
    # Add new mode
    if assessment_mode == 'ai_prioritized':
        # 80% semantic, 20% traditional
        semantic_result = self.get_semantic_score(candidate_data, job_data)
        traditional_result = self.get_traditional_score(candidate_data, job_data)
        
        if traditional_result:
            total_score = (
                semantic_result['semantic_score'] * 0.8 +
                traditional_result * 0.2
            )
        else:
            total_score = semantic_result['semantic_score']
    
    # ... existing modes
```

### 3. Custom Weighting by Job Type

```python
def get_job_type_weights(job_data: Dict) -> Dict:
    """Return custom weights based on job type"""
    
    job_title = job_data.get('title', '').lower()
    
    if 'professor' in job_title or 'academic' in job_title:
        return {
            'education_relevance': 0.50,
            'experience_relevance': 0.30,
            'skills_relevance': 0.15,
            'overall_quality_bonus': 0.05
        }
    elif 'engineer' in job_title or 'developer' in job_title:
        return {
            'education_relevance': 0.20,
            'experience_relevance': 0.50,
            'skills_relevance': 0.25,
            'overall_quality_bonus': 0.05
        }
    elif 'manager' in job_title or 'director' in job_title:
        return {
            'education_relevance': 0.25,
            'experience_relevance': 0.60,
            'skills_relevance': 0.10,
            'overall_quality_bonus': 0.05
        }
    else:
        # Default weights
        return {
            'education_relevance': 0.35,
            'experience_relevance': 0.45,
            'skills_relevance': 0.15,
            'overall_quality_bonus': 0.05
        }
```

### 4. Adding New Models

```python
# Add model registry
MODEL_REGISTRY = {
    'lightweight': 'all-MiniLM-L6-v2',
    'balanced': 'all-mpnet-base-v2', 
    'high_accuracy': 'all-roberta-large-v1',
    'multilingual': 'paraphrase-multilingual-MiniLM-L12-v2'
}

class UniversitySemanticEngine:
    def __init__(self, model_type: str = 'lightweight'):
        model_name = MODEL_REGISTRY.get(model_type, model_type)
        self.model = SentenceTransformer(model_name)
        
        # Adjust dimension based on model
        self.dimension = self.model.get_sentence_embedding_dimension()
```

### 5. Custom Data Processing

```python
def _extract_custom_fields(self, candidate_data: Dict) -> str:
    """Extract and process custom candidate fields"""
    
    custom_text = []
    
    # Process custom fields
    if 'certifications' in candidate_data:
        certs = candidate_data['certifications']
        if isinstance(certs, list):
            custom_text.extend(certs)
        elif isinstance(certs, str):
            custom_text.append(certs)
    
    if 'publications' in candidate_data:
        pubs = candidate_data['publications']
        custom_text.append(f"Publications: {pubs}")
    
    if 'awards' in candidate_data:
        awards = candidate_data['awards'] 
        custom_text.append(f"Awards: {awards}")
    
    return ' '.join(custom_text)
```

### 6. Advanced FAISS Configuration

```python
def setup_advanced_faiss_index(self, index_type: str = 'flat'):
    """Setup advanced FAISS indexing for large-scale deployment"""
    
    if index_type == 'flat':
        # Exact search (small datasets)
        index = faiss.IndexFlatIP(self.dimension)
        
    elif index_type == 'ivf':
        # Inverted file index (medium datasets)
        quantizer = faiss.IndexFlatIP(self.dimension)
        index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist=100)
        
    elif index_type == 'ivf_pq':
        # Product quantization (large datasets)
        quantizer = faiss.IndexFlatIP(self.dimension)
        index = faiss.IndexIVFPQ(quantizer, self.dimension, nlist=100, m=8, nbits=8)
        
    elif index_type == 'hnsw':
        # Hierarchical NSW (very large datasets)
        index = faiss.IndexHNSWFlat(self.dimension, 32)
        index.hnsw.efConstruction = 200
        
    return index
```

---

## Summary

The semantic scoring system provides a powerful, flexible, and scalable solution for AI-powered candidate assessment. Key takeaways:

‚úÖ **Core Files**: `semantic_engine.py` and `enhanced_assessment_engine.py`
‚úÖ **Database**: Three new fields in `candidates` table with proper indexing
‚úÖ **Customizable**: Weights, models, and assessment modes can be modified
‚úÖ **Scalable**: Supports batch processing and efficient similarity search
‚úÖ **Production Ready**: Comprehensive error handling and performance optimization

The system is designed to be both powerful out-of-the-box and highly customizable for specific organizational needs.